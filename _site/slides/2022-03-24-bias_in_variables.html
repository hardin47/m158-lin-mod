<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>An Example to Remember</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jo Hardin" />
    <meta name="date" content="2022-03-24" />
    <script src="2022-03-24-bias_in_variables_files/header-attrs-2.11/header-attrs.js"></script>
    <link href="2022-03-24-bias_in_variables_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="2022-03-24-bias_in_variables_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="2022-03-24-bias_in_variables_files/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="2022-03-24-bias_in_variables_files/panelset-0.2.6/panelset.js"></script>
  </head>
  <body>
    <textarea id="source">
class: right, top, my-title, title-slide

# An Example to Remember
### Jo Hardin
### March 24, 2022

---







## Bias in a model

```
talent ~ Normal (100, 15)
grades ~ Normal (talent, 15)
SAT ~ Normal (talent, 15)
```

College wants to admit students with 
&gt; `talent &gt; 115` 

... but the college only has access to `grades` and `SAT` which are noisy estimates of `talent`.

The example is taken directly (and mostly verbatim) from a blog by Aaron Roth [Algorithmic Unfairness Without Any Bias Baked In](http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html).  

---

## Plan for accepting students

* Run a regression on a training dataset (`talent` is known for existing students)
* Find a model which predicts `talent` based on `grades` and `SAT`
* Choose students for whom predicted `talent` is above 115

---

##  Flaw in the plan ...

* there are two populations of students, the Reds and Blues.  
   * Reds are the majority population (99%)  
   * Blues are a small minority population (1%)   
   
* the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above.   

* there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions

--

&gt; But there is one difference: the Blues have more money than the Reds, so they each take the **SAT twice**, and report only the highest of the two scores to the college. 

&gt; Taking the test twice results in a small but noticeable bump in the average SAT scores of the Blues, compared to the Reds.

---

##  Key insight:

&gt; The value of `SAT` means something different for the Reds versus the Blues

(They have different feature distributions.)

---

##  Let's see what happens ...

![](2022-03-24-bias_in_variables_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---

## Two models:



Red model (SAT taken once):

```
## # A tibble: 3 × 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)   33.4     0.152        220.       0
## 2 SAT            0.332   0.00149      223.       0
## 3 grades         0.333   0.00150      223.       0
```
Blue model (SAT is max score of two):

```
## # A tibble: 3 × 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   24.2      1.52        15.9 8.47e- 51
## 2 SAT            0.430    0.0154      27.9 6.53e-127
## 3 grades         0.291    0.0142      20.5 3.15e- 78
```

---

## New data

* Generate new data and use the **two** models above, separately. 

* How well do the models predict if a student has `talent` &gt; 115?

![](2022-03-24-bias_in_variables_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

---

## New data

.pull-left[
![](2022-03-24-bias_in_variables_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;
]


.pull-right[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; color &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; tpr &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; fpr &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; error &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; blue &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.510 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.044 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.113 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; red &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.504 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.037 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.109 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

`$$\texttt{tpr} = \frac{\mbox{true positives}}{\mbox{all who should}}$$`

`$$\texttt{fpr} = \frac{\mbox{false positives}}{\mbox{all who should not}}$$`

]

---

## **TWO** models doesn't seem right????

What if we fit only one model to the entire dataset?

After all, there are laws against using protected classes to make decisions (housing, jobs, credit, loans, college, etc.)


```
## # A tibble: 3 × 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)   33.4     0.151        221.       0
## 2 SAT            0.332   0.00148      224.       0
## 3 grades         0.334   0.00149      224.       0
```

(The coefficients kinda look like the red model...)
---

## How do the error rates change?

.pull-left[
![](2022-03-24-bias_in_variables_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;
]

.pull-right[

One model:
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; color &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; tpr &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; fpr &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; error &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; blue &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.613 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.063 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.113 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; red &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.502 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.037 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.109 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

Two separate models:
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; color &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; tpr &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; fpr &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; error &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; blue &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.510 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.044 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.113 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; red &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.504 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.037 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.109 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


]
---

##  What did we learn?

&gt; with two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations

* depending on the nature of the distribution difference, it can be either to the benefit or the detriment of the minority population

* no explicit human bias, either on the part of the algorithm designer or the data gathering process

* the problem is exacerbated if we artificially force the algorithm to be group blind

* well intentioned "fairness" regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time

---

## Simulate?

* different varying proportions
* effect due to variability
* effect due to SAT coefficient
* different number of times the blues get to take the test
* etc.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightlines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
